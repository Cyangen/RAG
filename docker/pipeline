FROM python:3.11.5 as base

# Use args
ARG MINIMUM_BUILD
ARG USE_CUDA
ARG USE_CUDA_VER
ARG PIPELINES_URLS
ARG PIPELINES_REQUIREMENTS_PATH

## Basis ##
ENV ENV=prod \
    PORT=9099 \
    # pass build args to the build
    MINIMUM_BUILD=${MINIMUM_BUILD} \
    USE_CUDA_DOCKER=${USE_CUDA} \
    USE_CUDA_DOCKER_VER=${USE_CUDA_VER}

# Install GCC and build tools. 
# These are kept in the final image to enable installing packages on the fly.
RUN apt-get update && \
    apt-get install -y gcc build-essential curl git tesseract-ocr poppler-utils ffmpeg libsm6 libxext6  -y && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install Python dependencies
COPY pipeline_requirements/requirements.txt .
COPY pipeline_requirements/requirements-minimum.txt .
RUN pip3 install uv
RUN if [ "$MINIMUM_BUILD" != "true" ]; then \
        if [ "$USE_CUDA_DOCKER" = "true" ]; then \
            pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/$USE_CUDA_DOCKER_VER --no-cache-dir; \
        else \
            pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir; \    
        fi \
    fi
RUN if [ "$MINIMUM_BUILD" = "true" ]; then \
        uv pip install --system -r requirements-minimum.txt --no-cache-dir; \
    else \
        uv pip install --system -r requirements.txt --no-cache-dir; \
    fi

# Install ollama (assuming it's available in the package repository)
RUN curl -fsSL https://ollama.com/install.sh | sh

# Ollama Serve + Pull Models
RUN ollama serve & sleep 5 && ollama pull phi3.5 && ollama pull nomic-embed-text && ollama pull llava-phi3 && ollama pull llava-llama3:8b

# pip install
COPY pipeline_requirements/python.txt .
RUN pip install -r python.txt

# Layer on for other components
FROM base AS app

ENV PIPELINES_URLS=${PIPELINES_URLS} \
    PIPELINES_REQUIREMENTS_PATH=${PIPELINES_REQUIREMENTS_PATH}

# Copy the application code

# Set the working directory (you need to specify a directory) + Make if not exist
WORKDIR /LLM

# Copy Local Dirs into Container
COPY ./to_docker .

# Need to incorporate running the 2 files once to pull any models needed (but need figure out GPU, or only pull dont run)
RUN python PULL_MODEL_SCRIPT.py

# Run a docker command if either PIPELINES_URLS or PIPELINES_REQUIREMENTS_PATH is not empty
RUN if [ -n "$PIPELINES_URLS" ] || [ -n "$PIPELINES_REQUIREMENTS_PATH" ]; then \
    echo "Running docker command with PIPELINES_URLS or PIPELINES_REQUIREMENTS_PATH"; \
    ./start.sh --mode setup; \
    fi

# Configure GRADIO Requirements
EXPOSE 7860
ENV GRADIO_SERVER_NAME="0.0.0.0"

# Expose the port
ENV HOST="0.0.0.0"
ENV PORT="9099"

# Update build file with btr container startup command
CMD ["sh", "-c", "ollama serve > /dev/null 2>&1 & python ./multimodal_rag/rag_gradio.py"]
