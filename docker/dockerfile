FROM python:3.11.5

# Update package list and install necessary packages
RUN apt-get update
RUN apt-get install sudo
RUN sudo apt-get install -y curl
RUN sudo apt-get install -y tesseract-ocr
RUN sudo apt-get install -y poppler-utils
RUN sudo apt-get install -y ffmpeg libsm6 libxext6  -y

# Install ollama (assuming it's available in the package repository)
RUN curl -fsSL https://ollama.com/install.sh | sh

# Ollama Serve + Pull Models
RUN ollama serve & sleep 5 && ollama pull phi3.5 && ollama pull nomic-embed-text && ollama pull llava-phi3 && ollama pull llama3.2-vision

# pip install
COPY requirements.txt .
RUN pip install -r requirements.txt

# Set the working directory (you need to specify a directory) + Make if not exist
WORKDIR /LLM

# Copy Local Dirs into Container
COPY ./to_docker .

# Need to incorporate running the 2 files once to pull any models needed (but need figure out GPU, or only pull dont run)
RUN python PULL_MODEL_SCRIPT.py

# Configure GRADIO Requirements
EXPOSE 7860
ENV GRADIO_SERVER_NAME="0.0.0.0"

# Default command (optional, but useful for debugging) (runs ollama serve, followed by bash)
CMD ["sh", "-c", "ollama serve > /dev/null 2>&1 & python ./multimodal_rag/rag_gradio.py > /dev/null 2>&1 & bash"]