{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "import torch\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"phi3.5\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "ans = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "print(ans)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create embeddingsclear\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
    "\n",
    "db = Chroma(persist_directory=\"./db-keef\",\n",
    "            embedding_function=embeddings)\n",
    "\n",
    "# # Create retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {\"k\": 3}\n",
    ")\n",
    "\n",
    "# # Create Ollama language model - Gemma 2\n",
    "local_llm = 'phi3.5'\n",
    "\n",
    "llm = ChatOllama(model=local_llm,\n",
    "                 keep_alive=0 , # was \"3h\"\n",
    "                 max_tokens=512,  \n",
    "                 temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"<|user|>Answer the question based only on the following context and extract out a meaningful answer. \\\n",
    "Please write in full sentences with correct spelling and punctuation. if it makes sense use lists. \\\n",
    "If the context doen't contain the answer, just respond that you are unable to find an answer. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}<|end|>\n",
    "<|assistant|>AI:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the RAG chain using LCEL with prompt printing and streaming output\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Function to ask questions\n",
    "def ask_question(question):\n",
    "    print(\"Answer:\\n\\n\", end=\" \", flush=True)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n",
    "        if user_question.lower() == 'quit':\n",
    "            break\n",
    "        answer = ask_question(user_question)\n",
    "        # print(\"\\nFull answer received.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
