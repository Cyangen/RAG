{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = Ollama(model='phi3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized prompts for different types of queries\n",
    "cv_prompt = PromptTemplate(\n",
    "    template=\"You are an expert in Computer Vision. Answer the query: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "sa_prompt = PromptTemplate(\n",
    "    template=\"You are an expert in System Architecture. Answer the query: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "empty_prompt = PromptTemplate(\n",
    "    template=\"You are a helpful AI Assistant. Answer the query: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLMChains for each specialization\n",
    "cv_chain = LLMChain(llm=llm, prompt=cv_prompt)\n",
    "sa_chain = LLMChain(llm=llm, prompt=sa_prompt)\n",
    "no_chain = LLMChain(llm=llm, prompt=empty_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a router with a classification logic\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"Decide if the query is about 'Computer Vision' or 'System Architecture': {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "router_chain = LLMChain(llm=llm, prompt=router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(query):\n",
    "    # Use the router_chain to classify the query\n",
    "    classification_result = router_chain.run(query)\n",
    "    \n",
    "    # Route based on classification\n",
    "    if \"Computer Vision\" in classification_result:\n",
    "        return cv_chain.run(query)\n",
    "    elif \"System Architecture\" in classification_result:\n",
    "        return sa_chain.run(query)\n",
    "    else:\n",
    "        return no_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is CLIP?\"\n",
    "basic_response = no_chain.run(query)\n",
    "routed_response = route_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" CLIP (Contrastive Language–Image Pre-Training) is an artificial intelligence model developed by researchers at Salesforce Labs, which has been shown to effectively understand and interpret both visuals and natural language simultaneously without separate image or text representations. It's a multi-modal pre-trained transformer that can perform tasks like zero-shot classification of images based on the descriptions provided in English sentences with remarkable accuracy.\\n\\nThe key features and abilities of CLIP include:\\n1. Multi-task learning capability, where it simultaneously learns to recognize visual objects from a large set (ImageNet) and understand textual content describing these object categories or attributes using contrastive language-image pre-training technique. This is done by mapping both images and their corresponding descriptions into the same space of latent vectors shared between them in an unsupervised manner, enabling CLIP to learn high-quality image representations that align with natural language semantics without explicit labeling data or fine-tuning on a specific task.\\n2. Zero-shot classification: This ability allows CLIP models can predict the object category present in images even if it wasn't explicitly labeled during training by analyzing textual descriptions and finding similarities between them, enabling generalization across new objects with little or no additional data provided to the model after pretraining.\\n3. Robustness: Despite being trained on a diverse dataset comprising millions of images from thousands of categories coupled with over 40 million sentence pairs describing these visuals in English language textual descriptions, CLIP's robust performance extends beyond its training distribution and generalizes well to unseen data as well.\\n4. Versatility: Aside from image classification tasks like zero-shot learning, the model can also be adapted for various other vision-related applications such as fine-grained categorization (distinguishing among subtle differences in similar objects), object detection and segmentation, visual question answering, etc., by integrating it with existing state-of-the-art architectures like Mask R-CNN or Faster R-CNN.\\n5. Pre-training data: CLIP is trained using the largest dataset ever created for vision-language models – consisting of 481,263 images and their corresponding natural language sentences from sources such as Wikipedia articles - enabling it to learn a diverse set of visual concepts along with rich linguistic descriptions spanning different domains.\\n\\nSalesforce Labs has made CLIP available publicly in the form of pre-trained models that can be fine-tuned on specific tasks or downstream applications, which allows users and researchers to leverage its capabilities for their unique objectives without training from scratch by reducing time and computational resources required.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Cut (Contrastive Language-Image Pre-training) was introduced by researchers from Salesforce Labs and Stanford University as a novel approach to understanding both visual content and textual descriptions simultaneously within a unified framework, referred to as 'CLIP' or Contrastive Language-Image Pre-training. The primary aim is facilitating zero-shot learning where the model can make accurate predictions about images even without any prior training on such examples through their understanding of related visual and textual content.\\n\\nAt its core, CLIP incorporates a diverse dataset comprising millions of image/text pairs sourced from various internet websites to learn correlations between different types of imagery across multiple domains (e.g., landscapes, animals) alongside the context that describes them in natural language sentences or captions provided by humans for those images on web platforms like Flickr and Wikipedia. CLIP employs a contrastive loss function called 'MixMatch,' which uses both labeled data with paired image/caption examples (for supervised learning part) as well as unlabeled mixed-data scenarios, such as random text descriptions of images or the reverse scenario where an arbitrary set of visual inputs are combined to generate a description. The contrastive loss helps CLIP learn robust and discriminative representations between both modalities - image content and language contexts that can be effectively applied in zero-shot learning settings (predicting attributes for unseen data), cross-lingual understanding, or few-shot/one-shot recognition tasks across different datasets.\\n\\nCLIP's versatility lies within its ability to handle a wide range of visual content and text description scenarios seamlessly while maintaining competitive performance in various image classification benchmark tests like the MS COCO dataset (multitask), Flickr, or more recently CIFAR-10. CLIP is available as an open source codebase on GitHub for researchers to utilize it across their computer vision and natural language processing tasks of interest while offering a promising foundation towards building generalized multi-task AI models capable of understanding both visual content and associated text in the same way that humans do, albeit with current limitations.\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query rewriting prompt\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    template=\"Rewrite the query to include references at the end: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "query_rewriter = LLMChain(llm=llm, prompt=rewrite_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The revised query to include references at the end could be:\n",
      "\n",
      "CLIP, which stands for Contrastive Language-Image Pre-training (also known as Cross-lingual and Multimodal pre-trained models), is a versatile neural network model developed by researchers at DeepMind with significant contributions from OpenAI. Drawing inspiration from vision networks like VGG or ResNets, alongside transformer architectures such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), CLIP uniquely combines visual perception with natural language understanding for multimodal tasks involving image-text pairs. It pretrains a vision encoder to understand images in conjunction with the textual context of those same visuals, using millions of paired examples from ImageNet and LSUN RGB datasets along with their corresponding captions (in different languages). This robust representation learning allows CLIP to achieve zero-shot classification tasks without additional fine-tuning or specialized training for unseen categories. The architecture behind CLIP includes:\n",
      " 1. Vision Encoder - Utilizing ResNet as the backbone, this component processes input images by extracting visual features using convolutional neural networks (CNNs) or other vision processing techniques like ViT. In CLIP's case [^2], it employs a modified version of VisualBERT (Lee et al., 2021), which is based on the Transformer architecture and designed for visual-language understanding tasks, as inspiration for its ResNet backbone.\n",
      " 2. Text Encoder - This component translates textual input into numerical representations that capture semantic meaning using transformer-based models similar to BERT or GPT [^1]. In CLIP's case, it utilizes the SPECTER encoder (Miyato et al., 2021), which has been modified for multimodal learning tasks.\n",
      " 3. Contrastive Loss Function - During training, a contrastive loss function plays an essential role in ensuring that both image and text representations learned by their respective encoders share similar meanings or semantics (in essence aligning visual perception with language comprehension). This process is called \"contrastive learning\" since it helps CLIP learn to distinguish between semantically related and dissimilar inputs. The contrastive loss function used in CLIP [^3] borrows from the SimCLR approach, which utilizes a cosine similarity-based measure for computing representations' compatibility scores during training (Chen et al., 2020).\n",
      "4. Multimodal Adaptation - In multimodal tasks that involve inputs from different sources such as images coupled with textual input, CLIP is equipped to adapt itself accordingly by learning a joint representation harmoniously incorporating information coming in both modalities—visual and linguistic together rather than treating them separately or prioritizing the dominance of one modality over another during decision-making.\n",
      "  In summary, CLIP is an innovative multimodal model capable of understanding diverse visual concepts within a language context by pretraining on vast amounts of image and text data using contrastive learning techniques for achieving high levels of generalization across different tasks involving the interplay between images and texts without any additional fine-tuning or specialized training.\n",
      "  References:\n",
      "   - [^1] Radford, Alec et al., \"Attention is all you need.\" JMLR 2018, vol. 49, no. 56, pp. 1–32 (October 2018). ISSN 1573-9936\n",
      "   - [^2] Lee, Kenton et al., \"VisualBERT: Pre-training contextualized convolutional language representations for visual question answering.\" arXiv preprints. arXiv:2104.07897 (April 2021). ISSN 1533-6057\n",
      "   - [^3] Miyato, Tomotsune et al., \"SimCLR: Self-training visual representation using contrastive prediction http://proceedings.mlr.press/v9/miyato18a/miyato18a-paper.pdf (Janu06 2020). ISSN 1533-7422\n",
      "   - [^4] Chen, Tianle et al., \"A simple baseline for contrastive representation learning.\" arXiv preprints. arXiv:2002.08207 (February 2020). ISSN 1533-6057\n",
      "   - [^5] Devlin, Jeremy et al., \"BERT: Pre-training of bidirectional transformers for language understanding.\" arXiv preprints. arXiv:1810.04889 (October 2019). ISSN 1533-6057\n",
      "   - [^6] Le et al., \"A simple approach to train very deep neural networks without gradient vanishing or exploding.\" arxiv preprints. arXiv:1309.6322 (September 2013). ISSN 1538-0215\n",
      "   - [^7] Simonyan, Karen et al., \"Very deep convolutional networks for large-scale image classification.\" In Advances in neural information processing systems, pp. 1468–1478 (December 2014). ISSN 1937-5938\n"
     ]
    }
   ],
   "source": [
    "rewritten_query = query_rewriter.run(routed_response)\n",
    "print(rewritten_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agentic Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_tool(input_text):\n",
    "    return f\"Rewritten query: {input_text}\"\n",
    "\n",
    "rewrite_tool = Tool(name=\"QueryRewriter\", func=rewrite_tool, description=\"Rewrite the query to include references at the end\")\n",
    "agent = initialize_agent([rewrite_tool], llm, agent=\"zero-shot-react-description\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Language-Image Pre-training (CLIP) is an advanced multimodal AI model created with the purpose of unifying visual perception, similar to that found in models like VGG networks or ResNets, and natural language understanding inspired by GPT and BERT. This integration allows CLIP not only to understand images but also their corresponding textual contexts effectively. \n",
      "\n",
      "During its pretraining process on millions of image-text pairs from ImageNet and LSUN RGB datasets as well as a diverse array of languages, it learns robust representations that are cross-lingual—meaning CLIP can work with various language inputs related to the images without being restricted or biased towards any specific one.\n",
      "\n",
      "CLIP's core components include: \n",
      "1. Vision Encoder - It employs ResNet as its backbone and processes raw pixel values into more abstract, universal visual features via CNN-based techniques (or variants like ViT). This encoder allows CLIP to grasp the essence of images without being dependent on specific language contexts during initial processing stages.\n",
      "   \n",
      "2. Text Encoder - It employs transformer architecture models—similar in spirit to BERT and GPT—to translate text inputs into numerical representations that capture semantic meanings, which is crucial for effectively relating visual cues with associated texts accurately as CLIP learns during the pretraining phase.\n",
      "   \n",
      "3. Contrastive Loss Function - This component ensures both image-derived and language context encoders learn to associate inputs that share similar meanings, helping them align semantically—thereby enhancing their ability in understanding complex multimodal tasks such as zero-shot learning where the model can classify images into categories it hasn't seen before based on text descriptions alone.\n",
      "   \n",
      "4. Multimodal Adaptation - CLIP has learned to adapt itself for dealing with different modalities—combining visual and linguistic inputs together, rather than treating them separately or prioritizing one over the other during decision-making processes which makes it highly versatile in multifaceted tasks.\n",
      "   \n",
      "CLIP's ability not just learns to understand diverse concepts within a language context but can also do zero-shot classification—a task where CLIP accurately classifies images into unseen categories without any additional fine-tuning or specialized training by leveraging its pretrained knowledge and generalizable understanding of languages. \n",
      "\n",
      "This is an innovative approach, making the model capable to work across various tasks involving diverse visual concepts within a language context while having high levels of generalized abilities for such multimodal interplays—all without additional fine-tuning or specialized training required in conventional models.\n"
     ]
    }
   ],
   "source": [
    "agent_rewritten_query = agent.run(routed_response)\n",
    "print(agent_rewritten_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_prompt = PromptTemplate(\n",
    "    template=\"Expand the following query by adding similar models: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "query_expander = LLMChain(llm=llm, prompt=expansion_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CLIP can be extended to incorporate similar models that have their unique features, adaptability, and architectural variations in handling multimodal data:\n",
      "\n",
      "1. **ViT (Vision Transformer):** This model uses transformers for vision processing rather than CNNs like ResNets or VGG networks used by CLIP's encoder. ViT treats images as sequences of patches and processes them using self-attention mechanisms similar to those in NLP, allowing it to capture global dependencies between image regions effectively.\n",
      "\n",
      "2. **SimCLR (Self-Supervised Learning Framework):** While not a single model like CLIP or transformer models used for language processing, SimCLR is another approach that learns visual representations by maximizing agreement with augmented versions of the same images while simultaneously minimizing disagreement among different examples in an unsupervised manner.\n",
      "\n",
      "3. **LXMERT (Language-Image Crossmodal Transformer):** LXMERT combines convolutional neural networks for processing image inputs and transformer architecture to handle textual information, akin to CLIP but with its distinct training objectives focusing on cross-linguistic understanding in multimodal tasks.\n",
      "   \n",
      "4. **MAE (Masked Autoencoder):** This model uses convolutions combined with self-attention mechanisms for vision processing and is trained similarly to SimCLR using contrastive learning, but instead of language pairs as inputs during pretraining like CLIP's text encoder does, it processes image patches.\n",
      "   \n",
      "5. **Megatron (Large Scale Transformers):** Although primarily a model for scaling transformer-based architectures to large-scale models and parallel training across multiple GPUs/TPUs, Megatron can be seen as an extension in terms of applying its techniques towards improving CLIP's multimodal understanding by leveraging massive amounts of data.\n",
      "   \n",
      "6. **BERT (Bidirectional Encoder Representations from Transformers):** While BERT is strictly for natural language processing and does not inherently include visual perception, it can be paired with CLIP or similar models to enhance textual understanding before feeding the information into a multimodal model like CLIP.\n",
      "   \n",
      "7. **SAM (Self-Attention Mechanism):** This architecture employs self-attention mechanisms and could potentially improve aspects of contrastive learning in image encoders, making it an alternative or supplementary approach to the ResNet backbone used by CLIP's vision component for aligning visual perception with language context.\n",
      "   \n",
      "8. **DeBERTa (Decoding-aware Enhanced BERT):** DeBERTa utilizes enhancing mechanisms like disentangled attention and cross-attetion to improve the performance of transformer models, which could be integrated into CLIP's text encoder architecture for better language understanding relatedness with visual inputs.\n",
      "   \n",
      "9. **Swin Transformer:** This model uses a novel multi-resolution self-attention mechanism that enables it to capture fine details as well as broader context simultaneously without the need of explicitly downsampling image patches like ViT, which might offer an alternative way to process visual input for CLIP's vision encoder.\n",
      "   \n",
      "10. **DINO (DIstillation IN: No Target Representation NEeded):** DINO is a self-supervised learning framework designed specifically with the goal of capturing image representations without relying on paired datasets, which can be beneficial for CLIP's training process by providing diverse and unpaired visual inputs.\n",
      "   \n",
      "By integrating these models or aspects into CLPI’s architecture where appropriate, we could potentially enhance its multimodal capabilities across different tasks that require understanding the interplay between images and textual information while leveraging contrastive learning methods to align their respective representations effectively without any fine-tuning for zero-shot classification.\n"
     ]
    }
   ],
   "source": [
    "expanded_query = query_expander.run(routed_response)\n",
    "print(expanded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_query_expansion(query):\n",
    "    expansion_map = {\n",
    "        \"CV\": [\"Object Detection\", \"Object Classification\", \"Image Segmentation\"]\n",
    "    }\n",
    "    terms = expansion_map.get(query, [])\n",
    "    return query + \" \" + \", \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = no_chain.run(\"CV\")\n",
    "expanded_query = custom_query_expansion(\"CV\")\n",
    "expanded_ai_response = no_chain.run(expanded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I\\'m sorry, but it seems like you might have made an error in your input since \"CV\" typically stands for \"CurriculCT,\" which is related to education or academic qualifications rather than being a standalone term that can be explained directly without context. \\n\\nIf the intention was asking about what \\'curriculum\\' (abbreviated as Cv) refers to, here it is an educational program consisting of subjects and learning outcomes designed by institutions for teaching students within their organization or society:\\n\\nCurriculum design involves deciding on a sequence of topics that aligns with the goals, mission, standards set by education bodies like IBO (International Baccalaureate), NCLB (No Child Left Behind in US policy context) and other relevant educational policies. It also includes choosing appropriate teaching methods to help students learn effectively from these materials over a certain period of time.\\n\\nCurriculum can be divided into four main components: content, learning experiences/activities, outcomes or goals, assessment (or evaluation). Content refers to the actual subject matter that needs to be taught and learned; it includes facts about historical events, concepts in mathematics, vocabulary words in languages etc. Learning activities are designed by educators as ways of teaching students these subjects so they can learn effectively – this could include lectures, discussions, problem-solving tasks or field trips for example.\\n\\nCurriculum goals refer to the specific skills and knowledge that a student is expected to gain from studying each subject; in other words, what \\'we want\\' students to know after completing their studies of this content area. These can often be summarized as ‘what do we hope our learners will achieve?’\\n\\nAssessment or evaluation refers to the ways by which educators measure whether a student has achieved these curriculum goals; assessments may take many forms, including tests, essays and presentations etc. Assessments can also provide valuable feedback for both students (to understand their progress) and teachers/education administrators (for improving teaching methods or content delivery).\\n\\nI hope this explanation gives you a clearer understanding of what \\'curriculum\\' means! If your query was different, I would be happy to help answer that instead.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" In computer vision and artificial intelligence (AI), tasks such as object detection, classification, and segmentation play crucial roles in interpreting visual information accurately and efficiently to support various applications like self-driving cars, medical image analysis, surveillance systems, etc. Here's a quick overview of these techniques:\\n\\n1. Object Detection: The process involves identifying different objects present within an image or video frame with their respective locations often represented by bounding boxes (rectangles). In object detection algorithms like YOLO and SSD, the aim is to output not only class labels but also spatial coordinates of each detected object in a given scene.\\n\\n2. Object Classification: This task focuses on assigning one or more classes/labels to an entire image based on its visual content without explicitly locating objects within it (unlike detection). The classification can be done at various levels, such as identifying whether the overall context of an image is a street scene, beach landscape etc. In convolutional neural networks like ResNet and VGG-16, this task becomes achievable through training on large datasets containing labeled images with multiple classes (e.g., animals or vehicles).\\n\\n3. Image Segmentation: This technique involves partitioning an image into meaningful segments that have distinct textures/objects. The goal of segmentation is to simplify representation making further processing like object detection more efficient and accurate by assigning each pixel in the image a label corresponding to its class (background, person, car etc.). Techniques for this can range from semantic segmentation where objects are categorized into broader categories without distinguishing their exact boundaries -to- instance segmentation which recognizes specific instances of multiple overlapping object classes. Algorithms like Mask R-CNN and U-Net have demonstrated remarkable performance in these tasks, especially when trained with extensive datasets such as COCO or PASCAL VOC containing images across different environments/conditions.\\n\\nIn essence, Object Detection involves identifying objects within an image along with their spatial coordinates; Object Classification assigns a single label to the whole scene based on its visual content without locating individual objects explicitly and Image Segmentation partitions an image into segments where each pixel in that segment is labeled as belonging to one of several object categories. All these tasks can be carried out using modern AI models, specifically deep convolutional neural networks (CNNs), which have shown great promise for achieving robust performance on challenging visual recognition problems across diverse applications and conditions when trained with extensive annotated datasets.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_ai_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
