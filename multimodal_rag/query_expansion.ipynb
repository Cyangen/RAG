{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class ParaphrasedQuery(BaseModel):\n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "\n",
    "    paraphrased_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique paraphrasing of the original question.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     17\u001b[0m     [\n\u001b[0;32m     18\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system),\n\u001b[0;32m     19\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     20\u001b[0m     ]\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keep_alive\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m llm_with_tools \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParaphrasedQuery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m query_analyzer \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm_with_tools \u001b[38;5;241m|\u001b[39m PydanticToolsParser(tools\u001b[38;5;241m=\u001b[39m[ParaphrasedQuery])\n",
      "File \u001b[1;32ms:\\LLM\\RAG\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1115\u001b[0m, in \u001b[0;36mBaseChatModel.bind_tools\u001b[1;34m(self, tools, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1110\u001b[0m     tools: Sequence[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, BaseMessage]:\n\u001b[1;32m-> 1115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \\\n",
    "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Return at least 3 versions of the question.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOllama(model=\"phi3.5\", temperature=0, keep_alive=0)\n",
    "llm_with_tools = llm.bind_tools([ParaphrasedQuery])\n",
    "query_analyzer = prompt | llm_with_tools | PydanticToolsParser(tools=[ParaphrasedQuery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here are three different ways to phrase your query for database search related to using multimodal models within an application pipeline, converting them into REST API format:\\n\\n1. \"How can I implement the integration of multiple modalities (visuals/text) in sequence processing and convert it into a Restful service?\"\\n2. \"What are the steps involved for chaining together different types of data inputs like images or text within an application, then exposing them as REST API endpoints?\"\\n3. \"Can you guide me through setting up multimodal models that process various forms (e.gypsum/text) in a sequence and translate it into Restful APIs for external accessibility?\"' additional_kwargs={} response_metadata={'model': 'phi3.5', 'created_at': '2025-02-27T03:17:27.1009606Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3715305500, 'load_duration': 1098271100, 'prompt_eval_count': 151, 'prompt_eval_duration': 220000000, 'eval_count': 145, 'eval_duration': 2396000000} id='run-39cc509a-f924-45d3-8067-ec3910388ea8-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ParaphrasedQuery)\n",
    "\n",
    "formatted_prompt = prompt.format(question=\"how to use multi-modal models in a chain and turn chain into a rest api\")\n",
    "response = llm.invoke(formatted_prompt)\n",
    "# parsed_output = parser.parse(response.content)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
